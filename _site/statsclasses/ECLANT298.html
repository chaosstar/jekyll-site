
<!DOCTYPE html> 
<!--
	Noam's theme, based on Readnik scriptogr.am theme
-->
<html lang="en">
<head>
  <title>Noam Ross | </title>
  <meta name="description" content=" " />
  <meta name="author" content="Noam Ross">
  <meta name="viewport" content="width=device-width; initial-scale=1.0; maximum-scale=1.0;">
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="ROBOTS" content="INDEX,FOLLOW">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">
  <meta property="og:url" content="">
  <meta property="og:site_name" content="Noam Ross">
  <link rel="icon" type="image/x-icon" href="/assets/themes/noamblog/atom.ico">
  <link href="/assets/themes/noamblog/css/style.css" rel="stylesheet" type="text/css" media="all">
  <style type="text/css">
    body {width: 600px !important;}
    .blog {width: 600px !important;}
    .container {margin: 0px 0; !important;}
    </style>
</head>
<body>
<div class="wrapper">
  <div class="blog">
 

      <div class="container">

        <div class="content">
          <p><strong>Official Description</strong>: This course is a PhD-level introduction to the Big Three frontiers of 21st century model-based statistical inference: (1) Bayesian inference and Markov Chain Monte Carlo (MCMC) estimation, (2) Mixed/hierarchical/multilevel generalized linear models (GLMM’s), and (3) formal model comparison using metrics like AIC, BIC and DIC. Each of these frontiers is intimately related to the others, and so it makes sense to teach them together. The alternative is to perpetuate the farrago of classical statistics. Having suffered through that farrago myself, I don’t wish it upon anyone else.</p>
<p>The course starts by focusing on “non-Bayesian” statistical inference relying upon maximum likelihood, but using Bayesian justifications and interpretations. Bayesian statistics is usually thought of as an advanced topic, completely divorced from what is usually known as “statistics.” This is a shame. It is easier to teach statistical inference with a Bayesian foundation. So I adopt a Bayesian framing at the start here, to make the course easier, not harder. Near the end of the course, when we meet some of the useful things that prior probability can do for us, it won’t seem ad hoc or magical. Bayes has been there all along.</p>
<p>The practical goals of the course are to teach students how to specify, fit and interpret GLM’s (generalized linear models) and GLMM’s (generalized linear mixed models), use AIC, understand how MCMC estimation works, and appreciate the powerful things Bayesian probability thinking can do for us.</p>
<p><strong>Instructors</strong>: Richard McElreath</p>
<p><strong>Other info and comments</strong>: Let us know in the comments</p>
<p><strong>Software used:</strong> R</p>
          


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'noamrossgh'; // required: replace example with your forum shortname
    var disqus_identifier = '/statsclasses/ECLANT298.html';
    var disqus_url = 'http://www.noamross.net/statsclasses/ECLANT298.html';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




        </div>
      </div>
</div>
</body>
</html>
